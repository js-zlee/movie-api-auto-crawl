import requests
import re
import json
from datetime import datetime, timedelta
import time
from urllib.parse import quote

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆæ–°æ‰‹å¯æ”¹ï¼Œæ³¨é‡Šå·²æ ‡æ¸…ï¼‰ ----------------------
# 1. å½±è§†æ¥å£æ ¸å¿ƒæœç´¢å…³é”®è¯ï¼ˆè¶Šå¤šï¼Œæ‰¾åˆ°çš„1å¹´å†…èµ„æºè¶Šå¤šï¼‰
SEARCH_KEYWORDS = [
    "å½±è§†æ¥å£ç«™ api.php/provide/vod/ å¯ç”¨ 2026",
    "zyapi å½±è§†èµ„æºæ¥å£ å…è´¹ æœ€æ–°",
    "lziapi caiji å½±è§†æ¥å£ æœ‰æ•ˆ",
    "å½±è§†API èµ„æºç«™ å…¬å¼€ 2026",
    "è‡ªåŠ¨é‡‡é›† å½±è§†æ¥å£é…ç½® JSON å¯ç”¨",
    "æœ€æ–°å½±è§†èµ„æºç«™ API æ¥å£ 2026"
]
# 2. å›½å†…å¯è®¿é—®çš„æœç´¢å¼•æ“å…¥å£åˆ—è¡¨ï¼ˆå¸¦1å¹´æ—¶é—´ç­›é€‰ï¼‰
# æ ¼å¼ï¼š(å¼•æ“åç§°, æœç´¢URLæ¨¡æ¿, æ—¶é—´ç­›é€‰å‚æ•°)
SEARCH_ENGINES = [
    # å¿…åº”ï¼šez7 ä»£è¡¨è¿‘1å¹´
    ("å¿…åº”", "https://cn.bing.com/search?q={}&first={}", "&filters=ex1:\"ez7\""),
    # ç™¾åº¦ï¼šcr=0_513 ä»£è¡¨è¿‘1å¹´
    ("ç™¾åº¦", "https://www.baidu.com/s?wd={}&pn={}", "&cr=0_513"),
    # 360æœç´¢ï¼št=1y ä»£è¡¨è¿‘1å¹´
    ("360æœç´¢", "https://www.so.com/s?q={}&pn={}", "&t=1y"),
    # æœç‹—æœç´¢ï¼šstime=1y ä»£è¡¨è¿‘1å¹´
    ("æœç‹—æœç´¢", "https://fanyi.sogou.com/websearch/searchList.jsp?query={}&page={}", "&stime=1y")
]
# 3. çˆ¬å–æ·±åº¦ï¼šæ¯ä¸ªå…³é”®è¯æœå‰3é¡µï¼ˆæ–°æ‰‹å»ºè®®2-3ï¼Œé¿å…è¢«å°ï¼‰
CRAWL_PAGE = 3
# 4. æ¥å£ç½‘å€åŒ¹é…è§„åˆ™ï¼ˆè¦†ç›–æ ¸å¿ƒæ ‡è¯†ï¼‰
API_PATTERN = re.compile(r'https?://[^\s)+?]+?(zy|api|lzi|caiji|cj)[^\s)*?]+?(api\.php/provide/vod/|api/json)')
# 5. è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé™ä½åçˆ¬ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.baidu.com/",
    "Accept-Language": "zh-CN,zh;q=0.9"
}
# 6. è¾“å‡ºJSONæ–‡ä»¶å
OUTPUT_FILE = "movie_api_list.json"
# 7. é˜²åçˆ¬é—´éš”ï¼ˆç§’ï¼‰
SLEEP_TIME = 1.5
# 8. æ¥å£éªŒæ´»é…ç½®ï¼ˆä¸¥æ ¼ç­›é€‰å¯ç”¨æ¥å£ï¼‰
VERIFY_TIMEOUT = 10  # æ¥å£å“åº”è¶…æ—¶æ—¶é—´ï¼ˆ10ç§’å†…æ²¡ååº”=æ— æ•ˆï¼‰
VERIFY_MIN_LENGTH = 50  # æ¥å£è¿”å›JSONæœ€å°é•¿åº¦ï¼ˆæ”¾å®½é™åˆ¶ï¼‰

# ---------------------- å·¥å…·å‡½æ•°ï¼šæ—¶é—´æ ¡éªŒï¼ˆå…œåº•è¿‡æ»¤1å¹´å†…èµ„æºï¼‰ ----------------------
def is_within_1_year(date_str):
    """æ ¡éªŒæ—¥æœŸå­—ç¬¦ä¸²æ˜¯å¦åœ¨è¿‘1å¹´å†…ï¼Œå…œåº•è¿‡æ»¤æ¼ç½‘çš„è¿‡æœŸèµ„æº"""
    try:
        # é€‚é…å¸¸è§æ—¥æœŸæ ¼å¼ï¼š2025-12-24 / 2025/12/24 / 2025.12.24
        date_formats = ["%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d"]
        for fmt in date_formats:
            try:
                post_date = datetime.strptime(date_str.strip(), fmt)
                break
            except:
                continue
        # è®¡ç®—1å¹´å‰çš„æ—¥æœŸ
        one_year_ago = datetime.now() - timedelta(days=365)
        return post_date >= one_year_ago
    except:
        # è§£æå¤±è´¥åˆ™é»˜è®¤ä¿ç•™ï¼ˆäº¤ç»™åç»­éªŒæ´»è¿‡æ»¤ï¼‰
        return True

# ---------------------- æ ¸å¿ƒå‡½æ•°1ï¼šæœç´¢1å¹´å†…çš„ç–‘ä¼¼æ¸ é“é“¾æ¥ ----------------------
def get_recent_channel_urls():
    channel_urls = set()
    print(f"===== å¼€å§‹ä½¿ç”¨ {len(SEARCH_ENGINES)} ä¸ªå›½å†…æœç´¢å¼•æ“ï¼Œæœç´¢è¿‘1å¹´å½±è§†æ¥å£æ¸ é“ =====")
    
    for engine_name, search_url_template, time_param in SEARCH_ENGINES:
        print(f"\nğŸ” ä½¿ç”¨ {engine_name} æœç´¢ï¼š")
        for keyword in SEARCH_KEYWORDS:
            # å¯¹å…³é”®è¯URLç¼–ç ï¼ˆé¿å…ä¸­æ–‡/ç‰¹æ®Šå­—ç¬¦æŠ¥é”™ï¼‰
            encoded_keyword = quote(keyword)
            print(f"  å…³é”®è¯ï¼š{keyword}")
            for page in range(1, CRAWL_PAGE + 1):
                # é€‚é…ä¸åŒå¼•æ“çš„åˆ†é¡µå‚æ•°
                if engine_name == "å¿…åº”":
                    page_param = (page - 1) * 10 + 1  # å¿…åº”åˆ†é¡µï¼š1ã€11ã€21...
                elif engine_name == "ç™¾åº¦":
                    page_param = (page - 1) * 10  # ç™¾åº¦åˆ†é¡µï¼š0ã€10ã€20...ï¼ˆpn=0æ˜¯ç¬¬1é¡µï¼‰
                elif engine_name == "360æœç´¢":
                    page_param = page  # 360åˆ†é¡µï¼š1ã€2ã€3...
                elif engine_name == "æœç‹—æœç´¢":
                    page_param = page  # æœç‹—åˆ†é¡µï¼š1ã€2ã€3...
                
                # æ‹¼æ¥å®Œæ•´æœç´¢URLï¼ˆå…³é”®è¯+åˆ†é¡µ+1å¹´æ—¶é—´ç­›é€‰ï¼‰
                search_url = f"{search_url_template.format(encoded_keyword, page_param)}{time_param}"
                
                try:
                    time.sleep(SLEEP_TIME)  # é˜²åçˆ¬ï¼Œæš‚åœ1.5ç§’
                    response = requests.get(search_url, headers=HEADERS, timeout=10)
                    response.encoding = response.apparent_encoding
                    
                    # æå–æœç´¢ç»“æœé“¾æ¥+å‘å¸ƒæ—¶é—´ï¼ˆäºŒæ¬¡è¿‡æ»¤ï¼‰
                    date_pattern = re.compile(r'<cite class="sb_csi_date">([\d\-/.]+)</cite>')
                    if engine_name == "ç™¾åº¦":
                        # ç™¾åº¦çš„å‘å¸ƒæ—¶é—´åœ¨<span class="c-color-gray2">æ ‡ç­¾é‡Œ
                        date_pattern = re.compile(r'<span class="c-color-gray2">([\d\-/.]+)</span>')
                    
                    link_pattern = re.compile(r'<a href="(https?://[^\s"]+)" target="_blank"')
                    # åŒ¹é…å‘å¸ƒæ—¶é—´å’Œé“¾æ¥
                    post_dates = date_pattern.findall(response.text)
                    all_links = link_pattern.findall(response.text)
                    
                    # éå†é“¾æ¥ï¼Œåªä¿ç•™1å¹´å†…çš„
                    for idx, link in enumerate(all_links):
                        # è¿‡æ»¤æ ¸å¿ƒæ ‡è¯†+äºŒæ¬¡æ—¶é—´è¿‡æ»¤
                        if any(word in link for word in ["zy", "api", "lzi", "caiji", "cj"]):
                            # æœ‰å‘å¸ƒæ—¶é—´åˆ™æ ¡éªŒï¼Œæ— åˆ™é»˜è®¤ä¿ç•™ï¼ˆäº¤ç»™éªŒæ´»ï¼‰
                            if idx < len(post_dates) and not is_within_1_year(post_dates[idx]):
                                print(f"  ç¬¬{page}é¡µï¼šè·³è¿‡è¿‡æœŸé“¾æ¥ [{link[:30]}...]")
                                continue
                            channel_urls.add(link)
                    print(f"    ç¬¬{page}é¡µï¼šè¿‡æ»¤åä¿ç•™ {len(channel_urls)} ä¸ª1å¹´å†…çš„æ¸ é“")
                except Exception as e:
                    print(f"    ç¬¬{page}é¡µçˆ¬å–å¤±è´¥ï¼š{str(e)}")
                    continue
    print(f"\n===== æœç´¢å®Œæˆï¼Œå…±è·å– {len(channel_urls)} ä¸ª1å¹´å†…çš„ç–‘ä¼¼æ¸ é“ =====")
    return list(channel_urls)

# ---------------------- æ ¸å¿ƒå‡½æ•°2ï¼šä»æ¸ é“çˆ¬å–æ¥å£ç½‘å€ ----------------------
def crawl_api_from_channels(channel_urls):
    all_api_urls = set()
    print(f"\n===== å¼€å§‹ä» {len(channel_urls)} ä¸ªæ¸ é“çˆ¬å–æ¥å£ç½‘å€ =====")
    for idx, channel in enumerate(channel_urls, 1):
        try:
            time.sleep(SLEEP_TIME)  # é˜²åçˆ¬ï¼Œæš‚åœ1.5ç§’
            response = requests.get(channel, headers=HEADERS, timeout=10)
            response.encoding = response.apparent_encoding
            # åŒ¹é…æ¥å£ç½‘å€
            api_matches = API_PATTERN.findall(response.text)
            api_urls = [match[0] + match[1] for match in api_matches]
            # å»é‡æ·»åŠ +ç®€å•æ¸…æ´—
            for url in api_urls:
                # ç®€å•æ¸…æ´—ï¼šå»æ‰å¤šä½™å­—ç¬¦ï¼ˆå¦‚æ‹¬å·ã€ç©ºæ ¼ï¼‰
                clean_url = url.strip().replace(")", "").replace("(", "")
                all_api_urls.add(clean_url)
            print(f"  æ¸ é“{idx}/{len(channel_urls)} [{channel[:30]}...]ï¼šçˆ¬å–åˆ° {len(api_urls)} ä¸ªæ¥å£")
        except Exception as e:
            print(f"  æ¸ é“{idx}/{len(channel_urls)} [{channel[:30]}...]ï¼šçˆ¬å–å¤±è´¥ {str(e)}")
            continue
    print(f"\n===== æ¸ é“çˆ¬å–å®Œæˆï¼Œå…±å‘ç° {len(all_api_urls)} ä¸ªåŸå§‹æ¥å£ç½‘å€ =====")
    return list(all_api_urls)

# ---------------------- æ ¸å¿ƒå‡½æ•°3ï¼šä¸¥æ ¼éªŒæ´»ï¼ˆä»…ä¿ç•™çœŸå¯ç”¨æ¥å£ï¼‰ ----------------------
def strict_verify_api_urls(api_urls):
    valid_urls = []
    print(f"\n===== å¼€å§‹ä¸¥æ ¼éªŒè¯ {len(api_urls)} ä¸ªæ¥å£çš„å¯ç”¨æ€§ =====")
    for idx, url in enumerate(api_urls, 1):
        try:
            # ä¸¥æ ¼éªŒæ´»æ¡ä»¶ï¼š10ç§’å†…å“åº” + 200çŠ¶æ€ç  + è¿”å›JSON + å“åº”å†…å®¹éç©º
            res = requests.get(
                url, 
                headers=HEADERS, 
                timeout=VERIFY_TIMEOUT,
                allow_redirects=True  # å…è®¸é‡å®šå‘ï¼ˆéƒ¨åˆ†æ¥å£ä¼šè·³è½¬ï¼‰
            )
            # æ¡ä»¶1ï¼šçŠ¶æ€ç 200
            if res.status_code != 200:
                print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆçŠ¶æ€ç {res.status_code}ï¼‰ï¼š{url}")
                continue
            # æ¡ä»¶2ï¼šè¿”å›JSONæ ¼å¼
            content_type = res.headers.get('Content-Type', '')
            if 'application/json' not in content_type and 'text/json' not in content_type:
                print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆéJSONå“åº”ï¼‰ï¼š{url}")
                continue
            # æ¡ä»¶3ï¼šå“åº”å†…å®¹éç©ºä¸”é•¿åº¦è¾¾æ ‡
            res_text = res.text.strip()
            if len(res_text) < VERIFY_MIN_LENGTH or res_text == "{}" or res_text == "[]":
                print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆç©ºJSONå“åº”ï¼‰ï¼š{url}")
                continue
            # æ‰€æœ‰æ¡ä»¶æ»¡è¶³ï¼Œä¿ç•™
            valid_urls.append(url)
            print(f"  {idx}/{len(api_urls)} âœ… æœ‰æ•ˆï¼š{url}")
        except requests.exceptions.Timeout:
            print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆè¶…æ—¶{VERIFY_TIMEOUT}ç§’ï¼‰ï¼š{url}")
        except requests.exceptions.ConnectionError:
            print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆè¿æ¥å¤±è´¥ï¼‰ï¼š{url}")
        except Exception as e:
            print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆæœªçŸ¥é”™è¯¯ï¼š{str(e)}ï¼‰ï¼š{url}")
    print(f"\n===== éªŒæ´»å®Œæˆï¼Œä»…ä¿ç•™ {len(valid_urls)} ä¸ªå¯ç”¨æ¥å£ =====")
    return valid_urls

# ---------------------- æ ¸å¿ƒå‡½æ•°4ï¼šä¿å­˜å¯ç”¨æ¥å£åˆ°JSON ----------------------
def save_valid_api_to_json(valid_urls):
    # æ„é€ æ ‡å‡†JSONç»“æ„ï¼ˆå’Œä½ ä¹‹å‰çš„é…ç½®ä¸€è‡´ï¼‰
    result = {
        "sites": [
            {
                "id": f"auto-{idx}",
                "key": f"1å¹´å†…æœ‰æ•ˆ-{idx}",
                "name": f"1å¹´å†…æœ‰æ•ˆ-{idx}",
                "api": url,
                "type": 2,
                "isActive": 1,
                "time": datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "+08:00",
                "isDefault": 0,
                "remark": f"GitHub Actionsè‡ªåŠ¨é‡‡é›†ï¼ˆè¿‘1å¹´èµ„æº+éªŒæ´»é€šè¿‡ï¼‰",
                "tags": ["1å¹´å†…æœ‰æ•ˆ", "è‡ªåŠ¨éªŒæ´»", "å¯ç”¨"],
                "priority": 0,
                "proxyMode": "none",
                "customProxy": ""
            }
            for idx, url in enumerate(valid_urls, 1)
        ],
        "exportTime": datetime.now().isoformat(),
        "total": len(valid_urls),
        "filters": {"search": None, "tags": None, "status": None}
    }
    # å†™å…¥JSONæ–‡ä»¶ï¼ˆè¦†ç›–æ—§æ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°å¯ç”¨æ¥å£ï¼‰
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… æœ€ç»ˆç»“æœï¼š{len(valid_urls)} ä¸ª1å¹´å†…å‘å¸ƒä¸”å½“å‰å¯ç”¨çš„æ¥å£å·²ä¿å­˜åˆ° {OUTPUT_FILE}")

# ---------------------- ä¸»å‡½æ•°ï¼šä¸²è”å…¨æµç¨‹ ----------------------
if __name__ == '__main__':
    start_time = datetime.now()
    print("===== å¼€å§‹ã€1å¹´å†…èµ„æº+å¤šå¼•æ“+è‡ªåŠ¨éªŒæ´»ã€‘å½±è§†æ¥å£é‡‡é›†å…¨æµç¨‹ =====")
    # æ­¥éª¤1ï¼šè·å–1å¹´å†…çš„æ¸ é“é“¾æ¥
    channel_urls = get_recent_channel_urls()
    if not channel_urls:
        print("âŒ æœªå‘ç°ä»»ä½•1å¹´å†…çš„æ¸ é“ï¼Œæµç¨‹ç»ˆæ­¢")
        # ç”Ÿæˆç©ºJSONï¼Œé¿å…GitHub Actionsæäº¤æŠ¥é”™
        save_valid_api_to_json([])
        exit()
    # æ­¥éª¤2ï¼šä»æ¸ é“çˆ¬å–æ¥å£
    raw_api_urls = crawl_api_from_channels(channel_urls)
    if not raw_api_urls:
        print("âŒ æœªä»æ¸ é“çˆ¬å–åˆ°ä»»ä½•æ¥å£ç½‘å€ï¼Œæµç¨‹ç»ˆæ­¢")
        save_valid_api_to_json([])
        exit()
    # æ­¥éª¤3ï¼šä¸¥æ ¼éªŒæ´»
    valid_api_urls = strict_verify_api_urls(raw_api_urls)
    # æ­¥éª¤4ï¼šä¿å­˜ç»“æœ
    save_valid_api_to_json(valid_api_urls)
    # ç»“æŸç»Ÿè®¡
    end_time = datetime.now()
    cost_time = (end_time - start_time).total_seconds()
    print(f"\n===== å…¨æµç¨‹å®Œæˆï¼è€—æ—¶ {cost_time:.1f} ç§’ï¼Œæœ€ç»ˆè·å– {len(valid_api_urls)} ä¸ªæœ‰æ•ˆå½±è§†æ¥å£ =====")
