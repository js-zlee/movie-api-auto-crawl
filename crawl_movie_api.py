import requests
import re
import json
from datetime import datetime, timedelta
import time
from urllib.parse import quote

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆæ–°æ‰‹å¯æ”¹ï¼Œæ³¨é‡Šå·²æ ‡æ¸…ï¼‰ ----------------------
# 1. å½±è§†æ¥å£æ ¸å¿ƒæœç´¢å…³é”®è¯ï¼ˆè¶Šå¤šï¼Œæ‰¾åˆ°çš„3ä¸ªæœˆå†…èµ„æºè¶Šå¤šï¼‰
SEARCH_KEYWORDS = [
    "å½±è§†æ¥å£ç«™ api.php/provide/vod/ å¯ç”¨",
    "zyapi å½±è§†èµ„æºæ¥å£ å…¬å¼€",
    "lziapi caiji å½±è§†æ¥å£",
    "å½±è§†API èµ„æºç«™ æœ€æ–°å¯ç”¨",
    "è‡ªåŠ¨é‡‡é›† å½±è§†æ¥å£é…ç½® JSON"
]
# 2. æœç´¢å¼•æ“å…¥å£ï¼ˆå¿…åº”ï¼Œå¸¦3ä¸ªæœˆæ—¶é—´ç­›é€‰ï¼‰
# &filters=ex1:"ez5" æ˜¯å¿…åº”è¯­æ³•ï¼šè¿‘3ä¸ªæœˆï¼›ez4=è¿‘1ä¸ªæœˆï¼Œez6=è¿‘6ä¸ªæœˆ
SEARCH_ENGINE = "https://cn.bing.com/search?q={}&first={}&filters=ex1:\"ez5\""
# 3. çˆ¬å–æ·±åº¦ï¼šæ¯ä¸ªå…³é”®è¯æœå‰2é¡µï¼ˆæ–°æ‰‹å»ºè®®1-2ï¼Œé¿å…è¢«å°ï¼‰
CRAWL_PAGE = 2
# 4. æ¥å£ç½‘å€åŒ¹é…è§„åˆ™ï¼ˆè¦†ç›–æ ¸å¿ƒæ ‡è¯†ï¼‰
API_PATTERN = re.compile(r'https?://[^\s)+?]+?(zy|api|lzi|caiji|cj)[^\s)*?]+?(api\.php/provide/vod/|api/json)')
# 5. è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé™ä½åçˆ¬ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://cn.bing.com/",
    "Accept-Language": "zh-CN,zh;q=0.9"
}
# 6. è¾“å‡ºJSONæ–‡ä»¶å
OUTPUT_FILE = "movie_api_list.json"
# 7. é˜²åçˆ¬é—´éš”ï¼ˆç§’ï¼‰
SLEEP_TIME = 1.5
# 8. æ¥å£éªŒæ´»é…ç½®ï¼ˆä¸¥æ ¼ç­›é€‰å¯ç”¨æ¥å£ï¼‰
VERIFY_TIMEOUT = 8  # æ¥å£å“åº”è¶…æ—¶æ—¶é—´ï¼ˆ8ç§’å†…æ²¡ååº”=æ— æ•ˆï¼‰
VERIFY_MIN_LENGTH = 100  # æ¥å£è¿”å›JSONæœ€å°é•¿åº¦ï¼ˆé¿å…ç©ºå“åº”ï¼‰

# ---------------------- å·¥å…·å‡½æ•°ï¼šæ—¶é—´æ ¡éªŒï¼ˆäºŒæ¬¡è¿‡æ»¤3ä¸ªæœˆå†…èµ„æºï¼‰ ----------------------
def is_within_3_months(date_str):
    """æ ¡éªŒæ—¥æœŸå­—ç¬¦ä¸²æ˜¯å¦åœ¨è¿‘3ä¸ªæœˆå†…ï¼Œå…œåº•è¿‡æ»¤æ¼ç½‘çš„è¿‡æœŸèµ„æº"""
    try:
        # é€‚é…å¸¸è§æ—¥æœŸæ ¼å¼ï¼š2025-12-24 / 2025/12/24 / 2025.12.24
        date_formats = ["%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d"]
        for fmt in date_formats:
            try:
                post_date = datetime.strptime(date_str.strip(), fmt)
                break
            except:
                continue
        # è®¡ç®—3ä¸ªæœˆå‰çš„æ—¥æœŸ
        three_months_ago = datetime.now() - timedelta(days=90)
        return post_date >= three_months_ago
    except:
        # è§£æå¤±è´¥åˆ™é»˜è®¤ä¿ç•™ï¼ˆäº¤ç»™åç»­éªŒæ´»è¿‡æ»¤ï¼‰
        return True

# ---------------------- æ ¸å¿ƒå‡½æ•°1ï¼šæœç´¢3ä¸ªæœˆå†…çš„ç–‘ä¼¼æ¸ é“é“¾æ¥ ----------------------
def get_recent_channel_urls():
    channel_urls = set()
    print(f"===== å¼€å§‹æœç´¢ã€è¿‘3ä¸ªæœˆã€‘çš„å½±è§†æ¥å£æ¸ é“ =====")
    for keyword in SEARCH_KEYWORDS:
        # å¯¹å…³é”®è¯URLç¼–ç ï¼ˆé¿å…ä¸­æ–‡/ç‰¹æ®Šå­—ç¬¦æŠ¥é”™ï¼‰
        encoded_keyword = quote(keyword)
        print(f"\nğŸ” æœç´¢å…³é”®è¯ï¼š{keyword}")
        for page in range(1, CRAWL_PAGE + 1):
            search_url = SEARCH_ENGINE.format(encoded_keyword, (page - 1) * 10 + 1)
            try:
                time.sleep(SLEEP_TIME)
                response = requests.get(search_url, headers=HEADERS, timeout=10)
                response.encoding = response.apparent_encoding
                # æå–æœç´¢ç»“æœé“¾æ¥+å‘å¸ƒæ—¶é—´ï¼ˆäºŒæ¬¡è¿‡æ»¤ï¼‰
                # å¿…åº”ç»“æœçš„å‘å¸ƒæ—¶é—´åœ¨<cite class="sb_csi_date">æ ‡ç­¾é‡Œ
                date_pattern = re.compile(r'<cite class="sb_csi_date">([\d\-/.]+)</cite>')
                link_pattern = re.compile(r'<a href="(https?://[^\s"]+)" target="_blank"')
                # åŒ¹é…å‘å¸ƒæ—¶é—´å’Œé“¾æ¥
                post_dates = date_pattern.findall(response.text)
                all_links = link_pattern.findall(response.text)
                # éå†é“¾æ¥ï¼Œåªä¿ç•™3ä¸ªæœˆå†…çš„
                for idx, link in enumerate(all_links):
                    # è¿‡æ»¤æ ¸å¿ƒæ ‡è¯†+äºŒæ¬¡æ—¶é—´è¿‡æ»¤
                    if any(word in link for word in ["zy", "api", "lzi", "caiji", "cj"]):
                        # æœ‰å‘å¸ƒæ—¶é—´åˆ™æ ¡éªŒï¼Œæ— åˆ™é»˜è®¤ä¿ç•™ï¼ˆäº¤ç»™éªŒæ´»ï¼‰
                        if idx < len(post_dates) and not is_within_3_months(post_dates[idx]):
                            print(f"  ç¬¬{page}é¡µï¼šè·³è¿‡è¿‡æœŸé“¾æ¥ [{link[:30]}...]")
                            continue
                        channel_urls.add(link)
                print(f"  ç¬¬{page}é¡µï¼šè¿‡æ»¤åä¿ç•™ {len(channel_urls)} ä¸ª3ä¸ªæœˆå†…çš„æ¸ é“")
            except Exception as e:
                print(f"  ç¬¬{page}é¡µçˆ¬å–å¤±è´¥ï¼š{str(e)}")
                continue
    print(f"\n===== æœç´¢å®Œæˆï¼Œå…±è·å– {len(channel_urls)} ä¸ª3ä¸ªæœˆå†…çš„ç–‘ä¼¼æ¸ é“ =====")
    return list(channel_urls)

# ---------------------- æ ¸å¿ƒå‡½æ•°2ï¼šä»æ¸ é“çˆ¬å–æ¥å£ç½‘å€ ----------------------
def crawl_api_from_channels(channel_urls):
    all_api_urls = set()
    print(f"\n===== å¼€å§‹ä» {len(channel_urls)} ä¸ªæ¸ é“çˆ¬å–æ¥å£ç½‘å€ =====")
    for idx, channel in enumerate(channel_urls, 1):
        try:
            time.sleep(SLEEP_TIME)
            response = requests.get(channel, headers=HEADERS, timeout=10)
            response.encoding = response.apparent_encoding
            # åŒ¹é…æ¥å£ç½‘å€
            api_matches = API_PATTERN.findall(response.text)
            api_urls = [match[0] + match[1] for match in api_matches]
            # å»é‡æ·»åŠ 
            for url in api_urls:
                # ç®€å•æ¸…æ´—ï¼šå»æ‰å¤šä½™å­—ç¬¦ï¼ˆå¦‚æ‹¬å·ã€ç©ºæ ¼ï¼‰
                clean_url = url.strip().replace(")", "").replace("(", "")
                all_api_urls.add(clean_url)
            print(f"  æ¸ é“{idx}/{len(channel_urls)} [{channel[:30]}...]ï¼šçˆ¬å–åˆ° {len(api_urls)} ä¸ªæ¥å£")
        except Exception as e:
            print(f"  æ¸ é“{idx}/{len(channel_urls)} [{channel[:30]}...]ï¼šçˆ¬å–å¤±è´¥ {str(e)}")
            continue
    print(f"\n===== æ¸ é“çˆ¬å–å®Œæˆï¼Œå…±å‘ç° {len(all_api_urls)} ä¸ªåŸå§‹æ¥å£ç½‘å€ =====")
    return list(all_api_urls)

# ---------------------- æ ¸å¿ƒå‡½æ•°3ï¼šä¸¥æ ¼éªŒæ´»ï¼ˆä»…ä¿ç•™çœŸå¯ç”¨æ¥å£ï¼‰ ----------------------
def strict_verify_api_urls(api_urls):
    valid_urls = []
    print(f"\n===== å¼€å§‹ä¸¥æ ¼éªŒè¯ {len(api_urls)} ä¸ªæ¥å£çš„å¯ç”¨æ€§ =====")
    for idx, url in enumerate(api_urls, 1):
        try:
            # ä¸¥æ ¼éªŒæ´»æ¡ä»¶ï¼š8ç§’å†…å“åº” + 200çŠ¶æ€ç  + è¿”å›JSON + å“åº”å†…å®¹éç©º
            res = requests.get(
                url, 
                headers=HEADERS, 
                timeout=VERIFY_TIMEOUT,
                allow_redirects=True  # å…è®¸é‡å®šå‘ï¼ˆéƒ¨åˆ†æ¥å£ä¼šè·³è½¬ï¼‰
            )
            # æ¡ä»¶1ï¼šçŠ¶æ€ç 200
            if res.status_code != 200:
                print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆçŠ¶æ€ç {res.status_code}ï¼‰ï¼š{url}")
                continue
            # æ¡ä»¶2ï¼šè¿”å›JSONæ ¼å¼
            content_type = res.headers.get('Content-Type', '')
            if 'application/json' not in content_type and 'text/json' not in content_type:
                print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆéJSONå“åº”ï¼‰ï¼š{url}")
                continue
            # æ¡ä»¶3ï¼šå“åº”å†…å®¹éç©ºä¸”é•¿åº¦è¾¾æ ‡
            res_text = res.text.strip()
            if len(res_text) < VERIFY_MIN_LENGTH or res_text == "{}" or res_text == "[]":
                print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆç©ºJSONå“åº”ï¼‰ï¼š{url}")
                continue
            # æ‰€æœ‰æ¡ä»¶æ»¡è¶³ï¼Œä¿ç•™
            valid_urls.append(url)
            print(f"  {idx}/{len(api_urls)} âœ… æœ‰æ•ˆï¼š{url}")
        except requests.exceptions.Timeout:
            print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆè¶…æ—¶{VERIFY_TIMEOUT}ç§’ï¼‰ï¼š{url}")
        except requests.exceptions.ConnectionError:
            print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆè¿æ¥å¤±è´¥ï¼‰ï¼š{url}")
        except Exception as e:
            print(f"  {idx}/{len(api_urls)} âŒ æ— æ•ˆï¼ˆæœªçŸ¥é”™è¯¯ï¼š{str(e)}ï¼‰ï¼š{url}")
    print(f"\n===== éªŒæ´»å®Œæˆï¼Œä»…ä¿ç•™ {len(valid_urls)} ä¸ªå¯ç”¨æ¥å£ =====")
    return valid_urls

# ---------------------- æ ¸å¿ƒå‡½æ•°4ï¼šä¿å­˜å¯ç”¨æ¥å£åˆ°JSON ----------------------
def save_valid_api_to_json(valid_urls):
    # æ„é€ æ ‡å‡†JSONç»“æ„ï¼ˆå’Œä½ ä¹‹å‰çš„é…ç½®ä¸€è‡´ï¼‰
    result = {
        "sites": [
            {
                "id": f"auto-{idx}",
                "key": f"3ä¸ªæœˆå†…æœ‰æ•ˆ-{idx}",
                "name": f"3ä¸ªæœˆå†…æœ‰æ•ˆ-{idx}",
                "api": url,
                "type": 2,
                "isActive": 1,
                "time": datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3] + "+08:00",
                "isDefault": 0,
                "remark": f"GitHub Actionsè‡ªåŠ¨é‡‡é›†ï¼ˆè¿‘3ä¸ªæœˆèµ„æº+éªŒæ´»é€šè¿‡ï¼‰",
                "tags": ["3ä¸ªæœˆå†…æœ‰æ•ˆ", "è‡ªåŠ¨éªŒæ´»", "å¯ç”¨"],
                "priority": 0,
                "proxyMode": "none",
                "customProxy": ""
            }
            for idx, url in enumerate(valid_urls, 1)
        ],
        "exportTime": datetime.now().isoformat(),
        "total": len(valid_urls),
        "filters": {"search": None, "tags": None, "status": None}
    }
    # å†™å…¥JSONæ–‡ä»¶ï¼ˆè¦†ç›–æ—§æ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°å¯ç”¨æ¥å£ï¼‰
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… æœ€ç»ˆç»“æœï¼š{len(valid_urls)} ä¸ª3ä¸ªæœˆå†…å‘å¸ƒä¸”å½“å‰å¯ç”¨çš„æ¥å£å·²ä¿å­˜åˆ° {OUTPUT_FILE}")

# ---------------------- ä¸»å‡½æ•°ï¼šä¸²è”å…¨æµç¨‹ ----------------------
if __name__ == '__main__':
    start_time = datetime.now()
    print("===== å¼€å§‹ã€3ä¸ªæœˆå†…èµ„æº+è‡ªåŠ¨éªŒæ´»ã€‘å½±è§†æ¥å£é‡‡é›†å…¨æµç¨‹ =====")
    # æ­¥éª¤1ï¼šè·å–3ä¸ªæœˆå†…çš„æ¸ é“é“¾æ¥
    channel_urls = get_recent_channel_urls()
    if not channel_urls:
        print("âŒ æœªå‘ç°ä»»ä½•3ä¸ªæœˆå†…çš„æ¸ é“ï¼Œæµç¨‹ç»ˆæ­¢")
        # ç”Ÿæˆç©ºJSONï¼Œé¿å…GitHub Actionsæäº¤æŠ¥é”™
        save_valid_api_to_json([])
        exit()
    # æ­¥éª¤2ï¼šä»æ¸ é“çˆ¬å–æ¥å£
    raw_api_urls = crawl_api_from_channels(channel_urls)
    if not raw_api_urls:
        print("âŒ æœªä»æ¸ é“çˆ¬å–åˆ°ä»»ä½•æ¥å£ç½‘å€ï¼Œæµç¨‹ç»ˆæ­¢")
        save_valid_api_to_json([])
        exit()
    # æ­¥éª¤3ï¼šä¸¥æ ¼éªŒæ´»
    valid_api_urls = strict_verify_api_urls(raw_api_urls)
    # æ­¥éª¤4ï¼šä¿å­˜ç»“æœ
    save_valid_api_to_json(valid_api_urls)
    # ç»“æŸç»Ÿè®¡
    end_time = datetime.now()
    cost_time = (end_time - start_time).total_seconds()
    print(f"\n===== å…¨æµç¨‹å®Œæˆï¼è€—æ—¶ {cost_time:.1f} ç§’ï¼Œæœ€ç»ˆè·å– {len(valid_api_urls)} ä¸ªæœ‰æ•ˆå½±è§†æ¥å£ =====")
